{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNV80WOqLkqbTYwtTjtzn4F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dCVPQHCXdooq"},"outputs":[],"source":["1.What is hypothesis testing in statistics?\n","-Hypothesis testing in statistics is a way to make informed decisions based on data. In Python, it's a breeze thanks to powerful libraries like scipy, statsmodels, and pingouin. Here's how it works:\n","\n","üîç What is Hypothesis Testing?\n","At its core, hypothesis testing is about testing a claim or idea about a population using sample data. You set up:\n","\n","Null Hypothesis (H‚ÇÄ): A default assumption (e.g., \"There is no difference between groups.\")\n","\n","Alternative Hypothesis (H‚ÇÅ): What you want to prove (e.g., \"There is a difference.\")\n","\n","Then, using statistical methods, you check whether the evidence from your data is strong enough to reject the null hypothesis.\n","üß™ Python Example Using scipy\n","Let‚Äôs say you're comparing test scores from two classes:\n","from scipy import stats\n","\n","class_a = [85, 87, 83, 90, 88]\n","class_b = [78, 82, 80, 79, 81]\n","\n","# Perform a t-test\n","t_stat, p_value = stats.ttest_ind(class_a, class_b)\n","\n","print(\"t-statistic:\", t_stat)\n","print(\"p-value:\", p_value)\n","If the p-value is lower than your threshold (commonly 0.05), you reject the null hypothesis, suggesting a significant difference between the two classes."]},{"cell_type":"markdown","source":["2.What is the null hypothesis, and how does it differ from the alternative hypothesis?\n","-üß© Null Hypothesis (H‚ÇÄ)\n","The null hypothesis is a default assumption that there is no effect, no difference, or nothing unusual happening in your data. It‚Äôs what we aim to test against.\n","\n","Example: If you're comparing the average height of two groups, the null might be: > ‚ÄúThe average height of Group A equals the average height of Group B.‚Äù\n","\n","üí° Alternative Hypothesis (H‚ÇÅ or Ha)\n","The alternative hypothesis is what you suspect might be true instead. It proposes that there is an effect, a difference, or something noteworthy.\n","Continuing the example: > ‚ÄúThe average height of Group A is different from the average height of Group B.‚Äù\n","\n","In Python Practice\n","Here‚Äôs how you'd test these hypotheses with a t-test:\n","from scipy import stats\n","\n","group_a = [160, 165, 170, 175, 180]\n","group_b = [155, 160, 165, 170, 175]\n","\n","# Null: means are equal\n","# Alternative: means are different\n","\n","t_stat, p_val = stats.ttest_ind(group_a, group_b)\n","\n","print(\"t-statistic:\", t_stat)\n","print(\"p-value:\", p_val)\n","If the p-value is less than a chosen significance level (say, 0.05), you'd reject the null hypothesis in favor of the alternative‚Äîconcluding there may indeed be a difference."],"metadata":{"id":"3ZhJ7-LCeQ9_"}},{"cell_type":"markdown","source":["3.What is the significance level in hypothesis testing, and why is it important?\n","-The significance level, usually denoted as Œ± (alpha), is a critical threshold in hypothesis testing. It represents the maximum probability of making a Type I error‚Äîthat is, rejecting the null hypothesis when it is actually true.\n","\n","üß† Why is it important?\n","Think of it as your ‚Äúrisk tolerance‚Äù:\n","\n","If you set Œ± = 0.05, you're saying you're willing to accept a 5% chance of falsely rejecting the null hypothesis.\n","\n","A smaller Œ± (like 0.01) means you want stronger evidence before rejecting H‚ÇÄ, reducing false positives but possibly increasing false negatives (Type II error).\n","\n","üìå In Python Testing Workflow\n","When performing a statistical test in Python using scipy.stats, you compare the p-value from the test against your chosen significance level:\n","from scipy import stats\n","\n","# Sample data\n","group1 = [10, 12, 13, 11, 10]\n","group2 = [14, 15, 13, 16, 14]\n","\n","# Perform a t-test\n","t_stat, p_val = stats.ttest_ind(group1, group2)\n","\n","# Set significance level\n","alpha = 0.05\n","\n","if p_val < alpha:\n","    print(\"Reject the null hypothesis\")\n","else:\n","    print(\"Fail to reject the null hypothesis\")\n","\n","So Œ± guides whether your test result is considered statistically significant. It‚Äôs like a gatekeeper deciding if your evidence is strong enough to warrant a change in belief."],"metadata":{"id":"ak7EiDXveqp8"}},{"cell_type":"markdown","source":["4. What does a P-value represent in hypothesis testing?\n","-In hypothesis testing, the p-value represents the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true. It helps determine the statistical significance of your results.\n","\n","Key Points:\n","Low p-value (< significance level, e.g., 0.05): Suggests strong evidence against the null hypothesis, so you may reject it.\n","High p-value (‚â• significance level): Suggests weak evidence against the null hypothesis, so you fail to reject it.\n","Significance Level (Œ±): A threshold (commonly 0.05) chosen to decide whether the p-value is small enough to reject the null hypothesis.\n","Example in Python:\n","\n","Here‚Äôs how you might calculate and interpret a p-value using Python:\n","\n","Copy the code\n","from scipy.stats import ttest_1samp\n","\n","# Example: One-sample t-test\n","data = [2.3, 2.5, 2.8, 3.0, 3.2]  # Sample data\n","population_mean = 2.5  # Null hypothesis: mean = 2.5\n","\n","# Perform t-test\n","t_stat, p_value = ttest_1samp(data, population_mean)\n","\n","# Output results\n","print(f\"T-statistic: {t_stat}\")\n","print(f\"P-value: {p_value}\")\n","\n","# Interpretation\n","if p_value < 0.05:  # Assuming Œ± = 0.05\n","    print(\"Reject the null hypothesis: Significant difference.\")\n","else:\n","    print(\"Fail to reject the null hypothesis: No significant difference.\")\n","\n","\n","This example uses a one-sample t-test to compare the sample mean to a population mean. The p_value helps decide whether the observed difference is statistically significant."],"metadata":{"id":"xN2W80kyfDyA"}},{"cell_type":"markdown","source":["5. How do you interpret the P-value in hypothesis testing?\n","-Interpreting the p-value in hypothesis testing is crucial for determining whether to reject or fail to reject the null hypothesis. Here's a concise explanation:\n","\n","What is a p-value?\n","\n","The p-value represents the probability of observing the test statistic (or something more extreme) under the assumption that the null hypothesis is true. It helps you decide whether the observed data provides enough evidence to reject the null hypothesis.\n","\n","How to interpret the p-value?\n","Small p-value (e.g., < 0.05): Strong evidence against the null hypothesis, so you reject it.\n","Large p-value (e.g., > 0.05): Weak evidence against the null hypothesis, so you fail to reject it.\n","Threshold (Œ±): The significance level (commonly 0.05) is the cutoff for deciding whether the p-value is \"small\" or \"large.\"\n","Example in Python\n","\n","Here‚Äôs how you might interpret the p-value in Python using a t-test:\n","\n","Copy the code\n","from scipy.stats import ttest_ind\n","\n","# Example data\n","group1 = [12, 14, 15, 16, 18]\n","group2 = [22, 24, 25, 26, 28]\n","\n","# Perform t-test\n","stat, p_value = ttest_ind(group1, group2)\n","\n","# Interpretation\n","alpha = 0.05  # Significance level\n","if p_value < alpha:\n","    print(f\"Reject the null hypothesis (p-value = {p_value:.4f})\")\n","else:\n","    print(f\"Fail to reject the null hypothesis (p-value = {p_value:.4f})\")\n","\n","Key Notes\n","The null hypothesis typically assumes no effect or no difference (e.g., means of two groups are equal).\n","The choice of significance level (Œ±) depends on the context of the study.\n","Always consider the practical significance of results, not just statistical significance.\n","\n","This approach ensures clarity and precision in hypothesis testing!"],"metadata":{"id":"RcEiasQVfQbl"}},{"cell_type":"markdown","source":["6. What are Type 1 and Type 2 errors in hypothesis testing?\n","-Type I and Type II errors are like two sides of a statistical coin flip‚Äîthey happen when our decisions about hypotheses go wrong. Let‚Äôs unpack both:\n","\n","‚ùå Type I Error (False Positive)\n","What it is: You reject the null hypothesis (H‚ÇÄ) when it's actually true.\n","\n","Analogy: Convicting an innocent person.\n","\n","In Python: This happens when the p-value is below your significance level (Œ±),, and you decide there's a real effect‚Äîbut it's just a fluke in the data.\n","alpha = 0.05\n","if p_val < alpha:\n","    # Risk of Type I error if H‚ÇÄ is actually true\n","    print(\"Rejecting H‚ÇÄ ‚Äî possible false positive\")\n","‚ùóÔ∏è Type II Error (False Negative)\n","What it is: You fail to reject the null hypothesis when it's actually false.\n","\n","Analogy: Letting a guilty person go free.\n","\n","In Python: If your p-value is above Œ±, you don't detect a difference‚Äîeven though one actually exists.\n","if p_val >= alpha:\n","    # Risk of Type II error if H‚ÇÄ is actually false\n","    print(\"Failing to reject H‚ÇÄ ‚Äî possible false negative\")\n","üí° Balancing Act\n","Lowering Œ± reduces Type I errors, but increases the chance of Type II errors.\n","\n","Increasing sample size helps reduce both types of errors.\n","\n","These errors are at the heart of understanding risk in hypothesis testing. Want to simulate them in code or see how sample size affects error rates? I can show you with a little Python magic.\n"],"metadata":{"id":"trBspA8sfkhG"}},{"cell_type":"markdown","source":["7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n","-One-Tailed vs. Two-Tailed Tests\n","1\n","2\n","In statistical significance testing, one-tailed and two-tailed tests are methods used to determine the relationship between variables based on the direction of the hypothesis.\n","\n","One-Tailed Test\n","\n","A one-tailed test is used when the hypothesis specifies a direction, either left or right. It tests whether a particular population parameter is larger or smaller than a predefined value. The critical region, where the null hypothesis is rejected, lies entirely on one side of the sampling distribution. For example, if we want to test whether a machine produces more than 1% defective products, we would use a one-tailed test.\n","\n","Example:\n","\n","Null Hypothesis (H0): There is no significant effect of students participating in a coding competition on their fear level.\n","\n","Alternative Hypothesis (H1): Participation in a coding competition decreases the fear level of students.\n","\n","Two-Tailed Test\n","\n","A two-tailed test is used when the hypothesis does not specify a direction. It tests whether the sample is greater or less than a range of values. The critical region is divided into two tails of the distribution. This method is used for null hypothesis testing, and if the estimated value exists in either tail, the null hypothesis is rejected.\n","\n","Example:\n","\n","Null Hypothesis (H0): There is no significant effect of a new bill passed on the loans of farmers.\n","\n","Alternative Hypothesis (H1): The new bill affects the loans of farmers, either increasing or decreasing them.\n","\n","Key Differences\n","\n","Direction: One-tailed tests specify a direction (greater or smaller), while two-tailed tests do not.\n","\n","Critical Region: One-tailed tests have the critical region on one side, whereas two-tailed tests have it on both sides.\n","\n","Significance Level: In one-tailed tests, the entire significance level (Œ±) is in one tail. In two-tailed tests, it is split between both tails\n","1\n","2\n",".\n","\n","Applications\n","\n","One-Tailed Test: Used for asymmetric distributions with a single tail, such as the chi-squared distribution.\n","\n","Two-Tailed Test: Used for symmetric distributions with two tails, such as the normal distribution\n","2\n",".\n","\n","Understanding the appropriate use of one-tailed and two-tailed tests is crucial for accurate statistical analysis and hypothesis testing."],"metadata":{"id":"4QA8xD_4gCmc"}},{"cell_type":"markdown","source":["8. What is the Z-test, and when is it used in hypothesis testing?\n","-The Z-test is a statistical method used to determine whether there's a significant difference between sample and population means‚Äîor between the means of two samples‚Äîwhen the population variance is known and/or the sample size is large.\n","\n","üß™ When to Use a Z-test\n","You'd reach for a Z-test when:\n","\n","Sample size is large (typically n ‚â• 30).\n","\n","Population standard deviation is known (which is rare in real-world data but ideal for theoretical testing).\n","\n","Your data is approximately normally distributed.\n","Typical scenarios:\n","\n","Testing if a sample mean differs from a known population mean.\n","\n","Comparing the means of two independent groups.\n","\n","‚öôÔ∏è Python Example Using statsmodels\n","import numpy as np\n","import statsmodels.stats.weightstats as sm\n","\n","# Sample data\n","data = [101, 103, 98, 100, 102, 104, 99]\n","pop_mean = 100  # Hypothetical population mean\n","\n","# Perform one-sample z-test\n","z_stat, p_val = sm.ztest(data, value=pop_mean)\n","\n","print(\"Z-statistic:\", z_stat)\n","print(\"p-value:\", p_val)\n","If the p-value is below your chosen significance level (say, 0.05), you reject the null hypothesis‚Äîsuggesting your sample mean is significantly different from the population mean."],"metadata":{"id":"cPNiOVOngRoY"}},{"cell_type":"markdown","source":["9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n","-The Z-score (or standard score) tells you how far a data point is from the population mean, measured in terms of standard deviations. In hypothesis testing, it helps determine whether your sample result is statistically unusual under the null hypothesis.\n","\n","üßÆ How to Calculate the Z-Score\n","For a single data point:\n","\n","ùëç\n","=\n","ùëã\n","‚àí\n","ùúá\n","ùúé\n","ùëã\n",": sample value\n","\n","ùúá\n",": population mean\n","\n","ùúé\n",": population standard deviation\n","\n","For a sample mean:\n","\n","ùëç\n","=\n","ùëã\n","Àâ\n","‚àí\n","ùúá\n","ùúé\n","/\n","ùëõ\n","ùëã\n","Àâ\n",": sample mean\n","\n","ùëõ\n",": sample size\n","\n","üêç Python Example: Z-score for Sample Mean\n","import numpy as np\n","\n","# Sample data\n","sample = [102, 98, 101, 100, 99]\n","population_mean = 100\n","population_std = 2\n","n = len(sample)\n","\n","# Calculate Z-score\n","sample_mean = np.mean(sample)\n","z_score = (sample_mean - population_mean) / (population_std / np.sqrt(n))\n","\n","print(\"Z-score:\", z_score)\n","üéØ What It Represents in Hypothesis Testing\n","A Z-score tells you how extreme your test statistic is under the null hypothesis.\n","\n","You use it to compute a p-value, which helps decide whether to reject\n","ùêª\n","0\n",".\n","\n","For example:\n","\n","If |Z| > 1.96 in a two-tailed test (with Œ± = 0.05), you'd likely reject the null hypothesis.\n","\n","Want to explore how Z-scores link to confidence intervals or simulate this for your own dataset? I'm ready to crunch the numbers with you."],"metadata":{"id":"5EmWf707gopn"}},{"cell_type":"markdown","source":["10. What is the T-distribution, and when should it be used instead of the normal distribution?\n","-The t-distribution (or Student‚Äôs t-distribution) is a probability distribution that's essential when you‚Äôre working with small sample sizes or when the population standard deviation is unknown‚Äîwhich, let‚Äôs be honest, is most of the time in real-world data.\n","\n","üìâ What is the T-Distribution?\n","It looks a lot like the normal distribution‚Äîsymmetric and bell-shaped‚Äîbut with fatter tails. Those fatter tails give it wiggle room to account for extra variability in smaller samples. As your sample size increases, the t-distribution converges toward the normal distribution.\n","üß™ When Should You Use It?\n","You use the t-distribution when:\n","\n","Sample size is small (typically n < 30)\n","\n","The population standard deviation (œÉ) is unknown\n","\n","You assume your data is approximately normally distributed\n","\n","Common situations:\n","\n","Estimating the mean of a population from a small sample\n","\n","Comparing two sample means (like in a t-test)\n","üêç Python Example Using scipy.stats.ttest_1samp\n","from scipy import stats\n","\n","# Sample data\n","sample = [22, 25, 20, 24, 23]\n","population_mean = 20\n","\n","# One-sample t-test\n","t_stat, p_val = stats.ttest_1samp(sample, population_mean)\n","\n","print(\"T-statistic:\", t_stat)\n","print(\"p-value:\", p_val)\n","\n","This test checks if the sample mean significantly differs from the population mean using the t-distribution under the hood.\n","\n","üìå Rule of Thumb\n","Use the t-distribution when sample size is small and population variance is unknown. Use the normal distribution when sample size is large and you know the population standard deviation."],"metadata":{"id":"z09_jktPhCYH"}},{"cell_type":"markdown","source":["11. What is the difference between a Z-test and a T-test?\n","-The z-test and t-test are both statistical tests used to compare means, but they differ in their assumptions and use cases. Here's a concise explanation of the differences, particularly in the context of Python:\n","\n","1. Key Differences\n","\n","Z-Test:\n","\n","Used when the population standard deviation is known.\n","Assumes the sample size is large (typically ( n > 30 )) or the data is normally distributed.\n","More appropriate for large datasets.\n","\n","T-Test:\n","\n","Used when the population standard deviation is unknown.\n","Works well with smaller sample sizes (( n \\leq 30 )).\n","Relies on the sample standard deviation as an estimate of the population standard deviation.\n","2. Python Implementation\n","\n","Python provides libraries like scipy and statsmodels to perform these tests. Here's how you can implement them:\n","\n","Z-Test\n","Copy the code\n","from statsmodels.stats.weightstats import ztest\n","\n","# Example: Perform a one-sample z-test\n","data = [12, 15, 14, 10, 13, 14, 15, 16]\n","z_stat, p_value = ztest(data, value=13)  # Compare sample mean to 13\n","print(f\"Z-Statistic: {z_stat}, P-Value: {p_value}\")\n","\n","T-Test\n","Copy the code\n","from scipy.stats import ttest_1samp\n","\n","# Example: Perform a one-sample t-test\n","data = [12, 15, 14, 10, 13, 14, 15, 16]\n","t_stat, p_value = ttest_1samp(data, popmean=13)  # Compare sample mean to 13\n","print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n","\n","3. When to Use Which\n","Use a z-test if you have a large sample size or know the population standard deviation.\n","Use a t-test for smaller sample sizes or when the population standard deviation is unknown.\n","\n","Both tests are widely used in hypothesis testing, and Python makes it easy to implement them with just a few lines of code!"],"metadata":{"id":"hLeLGUYBhclH"}},{"cell_type":"markdown","source":["12. What is the T-test, and how is it used in hypothesis testing?\n","-The t-test is a statistical test used to compare the means of groups to see if they‚Äôre significantly different from one another. It‚Äôs especially handy when you don‚Äôt know the population standard deviation and/or have a small sample size.\n","\n","üéØ Types of T-Tests\n","One-sample t-test: Compares the sample mean to a known population mean.\n","\n","Two-sample (independent) t-test: Compares the means of two independent groups.\n","\n","Paired (dependent) t-test: Compares means from the same group at different times (like before/after a treatment).\n","üêç Python Examples with scipy.stats\n","1. One-Sample T-Test\n","from scipy import stats\n","\n","sample = [52, 55, 50, 53, 54]\n","pop_mean = 50\n","\n","t_stat, p_val = stats.ttest_1samp(sample, pop_mean)\n","print(\"T-statistic:\", t_stat)\n","print(\"P-value:\", p_val)\n","2. Two-Sample T-Test\n","group1 = [102, 100, 98, 105]\n","group2 = [95, 96, 94, 97]\n","\n","t_stat, p_val = stats.ttest_ind(group1, group2)\n","print(\"T-statistic:\", t_stat)\n","print(\"P-value:\", p_val)\n","3. Paired T-Test\n","before = [85, 90, 88, 92]\n","after = [87, 91, 89, 95]\n","\n","t_stat, p_val = stats.ttest_rel(before, after)\n","print(\"T-statistic:\", t_stat)\n","print(\"P-value:\", p_val)\n","üß† Interpreting Results\n","If p-value < Œ± (usually 0.05): You reject the null hypothesis ‚Äî the difference is statistically significant.\n","\n","If p-value ‚â• Œ±: You fail to reject the null ‚Äî not enough evidence to prove a difference."],"metadata":{"id":"0q7anRKhho7M"}},{"cell_type":"markdown","source":["13. What is the relationship between Z-test and T-test in hypothesis testing?\n","-The z-test and t-test are both statistical methods used in hypothesis testing to determine whether there is a significant difference between sample data and a population parameter or between two sample groups. Here's how they are related and differ, particularly in the context of Python:\n","\n","Relationship Between Z-Test and T-Test\n","\n","Purpose:\n","\n","Both tests assess hypotheses about means.\n","They are used to compare sample data to a population mean or compare two sample means.\n","\n","Key Difference:\n","\n","Z-Test: Used when the population standard deviation ($$\\sigma$$) is known or the sample size is large (typically $$n > 30$$).\n","T-Test: Used when the population standard deviation is unknown and the sample size is small ($$n \\leq 30$$). It accounts for additional uncertainty by using the t-distribution.\n","\n","Underlying Distribution:\n","\n","Z-Test assumes the data follows a normal distribution.\n","T-Test uses the t-distribution, which is similar to the normal distribution but has heavier tails (to account for small sample sizes).\n","\n","Python Implementation:\n","\n","Both tests can be implemented using libraries like scipy.stats.\n","Python Examples\n","1. Z-Test (Using statsmodels.stats.weightstats.ztest):\n","Copy the code\n","from statsmodels.stats.weightstats import ztest\n","\n","# Example: One-sample Z-test\n","data = [12, 14, 15, 13, 16, 14, 15]\n","z_stat, p_value = ztest(data, value=14)  # Test if mean is 14\n","print(f\"Z-Statistic: {z_stat}, P-Value: {p_value}\")\n","\n","2. T-Test (Using scipy.stats.ttest_1samp or ttest_ind):\n","Copy the code\n","from scipy.stats import ttest_1samp, ttest_ind\n","\n","# Example: One-sample T-test\n","data = [12, 14, 15, 13, 16, 14, 15]\n","t_stat, p_value = ttest_1samp(data, popmean=14)  # Test if mean is 14\n","print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n","\n","# Example: Two-sample T-test\n","data1 = [12, 14, 15, 13, 16, 14, 15]\n","data2 = [10, 11, 12, 13, 14, 15, 16]\n","t_stat, p_value = ttest_ind(data1, data2)  # Compare two sample means\n","print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n","\n","Summary\n","Use z-test for large samples or when population standard deviation is known.\n","Use t-test for small samples or when population standard deviation is unknown.\n","Both tests are easily implemented in Python using libraries like scipy and statsmodels."],"metadata":{"id":"VRqgTx1yiQFZ"}},{"cell_type":"markdown","source":["14. What is a confidence interval, and how is it used to interpret statistical results?\n","-A confidence interval (CI) is like a smart estimate with a built-in margin of error. It gives you a range of values that's likely to contain the true population parameter (like a mean or proportion), based on your sample data.\n","\n","üéØ What Does a Confidence Interval Tell You?\n","If you calculate a 95% confidence interval for a population mean and get, say, [48.5, 53.2], it means: > \"We‚Äôre 95% confident that the true population mean lies between 48.5 and 53.2.\"\n","\n","It doesn't guarantee that the true value is within the interval, but it means that if we repeated the study 100 times, about 95 of those intervals would capture the true mean.\n","üêç How to Calculate a Confidence Interval in Python\n","Here‚Äôs a quick way to do it using scipy.stats:\n","import numpy as np\n","from scipy import stats\n","\n","# Sample data\n","data = [48, 52, 50, 53, 51]\n","n = len(data)\n","mean = np.mean(data)\n","std_err = stats.sem(data)  # Standard error of the mean\n","\n","# 95% confidence interval\n","confidence = 0.95\n","h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n","ci_lower = mean - h\n","ci_upper = mean + h\n","\n","print(f\"Mean: {mean:.2f}\")\n","print(f\"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n","üß† Why Confidence Intervals Matter\n","They help you:\n","\n","Understand precision: Narrow CIs mean more precise estimates.\n","\n","Assess significance: If a CI for a mean difference doesn‚Äôt include 0, the result is likely statistically significant.\n","\n","Communicate uncertainty clearly in scientific reports."],"metadata":{"id":"EV1IOrXkia6W"}},{"cell_type":"markdown","source":["15. What is the margin of error, and how does it affect the confidence interval?\n","-The margin of error is the \"plus or minus\" buffer that tells you how much uncertainty is in your estimate from sample data. It defines the range around a sample statistic (like a mean) that likely includes the true population parameter.\n","\n","üéØ What Does Margin of Error Represent?\n","Let‚Äôs say your survey gives a sample mean of 50, and the margin of error is ¬±2. That means you're fairly confident (usually 95%) that the true population mean lies between 48 and 52.\n","\n","The larger the margin of error, the less precise the estimate.\n","üßÆ How Is It Calculated?\n","For a confidence interval of a mean:\n","\n","Margin¬†of¬†Error\n","=\n","ùë°\n","‚àó\n","√ó\n","Standard¬†Error\n","ùë°\n","‚àó\n",": The critical value from the t-distribution (depends on confidence level and sample size)\n","\n","Standard Error: Standard deviation of the sample divided by the square root of the sample size\n","python example------\n","import numpy as np\n","from scipy import stats\n","\n","data = [48, 52, 50, 53, 51]\n","n = len(data)\n","mean = np.mean(data)\n","std_err = stats.sem(data)\n","\n","# 95% confidence level\n","confidence = 0.95\n","t_critical = stats.t.ppf((1 + confidence) / 2, df=n-1)\n","\n","margin_of_error = t_critical * std_err\n","ci_lower = mean - margin_of_error\n","ci_upper = mean + margin_of_error\n","\n","print(f\"Mean: {mean:.2f}\")\n","print(f\"Margin of Error: ¬±{margin_of_error:.2f}\")\n","print(f\"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n","üìå Why It Matters\n","Smaller margin of error = more reliable estimate\n","\n","Influenced by sample size and variability: larger samples = smaller error\n","\n","Helps you interpret how much you can \"trust\" your results"],"metadata":{"id":"2yKMq_7cizrB"}},{"cell_type":"markdown","source":["16. How is Bayes' Theorem used in statistics, and what is its significance?\n","-Bayes‚Äô Theorem is the ultimate statistical plot twist‚Äîit lets you update the probability of a hypothesis as more evidence or data becomes available. In everyday stats, it's essential for reasoning under uncertainty, and in Python, it becomes a powerful tool for everything from spam filters to medical diagnostics to machine learning models.\n","\n","üéØ The Formula\n","Bayes‚Äô Theorem is mathematically expressed as:\n","ùëÉ\n","(\n","ùê¥\n","‚à£\n","ùêµ\n",")\n","=\n","ùëÉ\n","(\n","ùêµ\n","‚à£\n","ùê¥\n",")\n","‚ãÖ\n","ùëÉ\n","(\n","ùê¥\n",")\n","ùëÉ\n","(\n","ùêµ\n",")\n","Where:\n","\n","ùëÉ\n","(\n","ùê¥\n","‚à£\n","ùêµ\n",")\n",": Posterior probability (probability of A given B has occurred)\n","\n","ùëÉ\n","(\n","ùêµ\n","‚à£\n","ùê¥\n",")\n",": Likelihood (probability of B given A is true)\n","\n","ùëÉ\n","(\n","ùê¥\n",")\n",": Prior probability of A\n","\n","ùëÉ\n","(\n","ùêµ\n",")\n",": Total probability of B\n","üß† Why It Matters\n","It flips your perspective: you start with a belief (prior), then update it with new evidence (likelihood), resulting in a new belief (posterior). That‚Äôs incredibly useful in:\n","\n","Medical testing: ‚ÄúGiven a positive test result, what‚Äôs the probability the patient actually has the disease?‚Äù\n","\n","Spam detection: ‚ÄúGiven these keywords, how likely is this email to be spam?‚Äù\n","\n","Machine learning: Naive Bayes classifiers rely on Bayes‚Äô rule.\n","üêç Python Example: Diagnosing a Condition\n","Let‚Äôs say:\n","\n","1% of people have a rare disease (prior)\n","\n","A test is 99% accurate (both sensitivity and specificity)\n","# Probabilities\n","P_disease = 0.01\n","P_no_disease = 1 - P_disease\n","P_positive_given_disease = 0.99\n","P_positive_given_no_disease = 0.01\n","\n","# Bayes' Theorem\n","P_positive = (P_positive_given_disease * P_disease) + \\\n","             (P_positive_given_no_disease * P_no_disease)\n","P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n","\n","print(f\"Probability of having the disease given a positive result: {P_disease_given_positive:.4f}\")\n","Even though the test is highly accurate, the actual probability of having the disease given a positive result is lower than you'd think‚Äîbecause the disease is rare! That‚Äôs the Bayes‚Äô perspective in action."],"metadata":{"id":"KqSSJu7UjO9J"}},{"cell_type":"markdown","source":["17. What is the Chi-square distribution, and when is it useed?\n","-The Chi-square distribution (œá¬≤ distribution) is a fundamental building block in statistics. It's used primarily for categorical data analysis, especially when you're interested in frequencies‚Äîhow often something happens.\n","\n","üéØ What Is the Chi-square Distribution?\n","It‚Äôs a right-skewed distribution that becomes more symmetric as the degrees of freedom increase. It arises when you sum the squares of independent standard normal variables. This makes it perfect for testing how well observed data fit expectations.\n","üß™ When Is It Used?\n","You use the Chi-square distribution in situations like:\n","\n","Goodness-of-Fit Test Checks if a sample matches a population distribution. Example: Does a die produce each number equally often?\n","\n","Test of Independence Evaluates whether two categorical variables are independent. Example: Is gender independent of voting preference?\n","\n","Test for Homogeneity Compares distributions across different populations. Example: Are color preferences the same across age groups?\n","üêç Python Example: Chi-Square Test of Independence\n","Using scipy.stats:\n","import numpy as np\n","from scipy.stats import chi2_contingency\n","\n","# Contingency table: rows = gender, columns = vote preference\n","data = np.array([[30, 10],  # e.g., Male: Yes/No\n","                 [20, 40]]) # e.g., Female: Yes/No\n","\n","chi2, p, dof, expected = chi2_contingency(data)\n","\n","print(\"Chi-square statistic:\", chi2)\n","print(\"Degrees of freedom:\", dof)\n","print(\"P-value:\", p)\n","print(\"Expected frequencies:\\n\", expected)\n","If the p-value is below your significance level (say 0.05), you reject the null hypothesis‚Äîsuggesting a relationship exists between the variables."],"metadata":{"id":"j_kVZNOpjn8W"}},{"cell_type":"markdown","source":["18. What is the Chi-square goodness of fit test, and how is it applied?\n","-Chi-Square Goodness of Fit Test in Python\n","1\n","2\n","3\n","The chisquare method from the scipy.stats module is used to perform a Chi-Square Goodness of Fit Test in Python. This test evaluates whether observed categorical data matches an expected distribution.\n","\n","Example:\n","\n","from scipy.stats import chisquare\n","\n","# Observed and expected frequencies\n","observed = [50, 60, 40, 47, 53]\n","expected = [50, 50, 50, 50, 50]\n","\n","# Perform the Chi-Square Goodness of Fit Test\n","statistic, p_value = chisquare(f_obs=observed, f_exp=expected)\n","\n","print(f\"Chi-Square Statistic: {statistic}\")\n","print(f\"P-Value: {p_value}\")\n","Output:\n","\n","Chi-Square Statistic: 4.36\n","P-Value: 0.35947\n","Explanation:\n","\n","Chi-Square Statistic quantifies the difference between observed and expected frequencies.\n","\n","P-Value determines the significance of the result. If p_value < 0.05, the null hypothesis (data follows the expected distribution) is rejected.\n","\n","Important Considerations:\n","\n","Degrees of Freedom: The test uses n-1 degrees of freedom, where n is the number of categories.\n","\n","Null Hypothesis (H‚ÇÄ): The observed data follows the expected distribution.\n","\n","Alternative Hypothesis (H‚ÇÅ): The observed data does not follow the expected distribution.\n","\n","Limitations:\n","\n","Ensure that all expected frequencies are greater than 5 for reliable results.\n","\n","For large datasets or complex distributions, additional statistical methods may be required."],"metadata":{"id":"2wDWB62yj_PZ"}},{"cell_type":"markdown","source":["19. What is the F-distribution, and when is it used in hypothesis testing?\n","-The F-distribution is a continuous probability distribution that arises frequently in statistics, particularly in the context of comparing variances. It is asymmetric and skewed to the right, with values ranging from 0 to infinity. The shape of the F-distribution depends on two parameters: the degrees of freedom for the numerator ($$df_1$$) and the denominator ($$df_2$$).\n","\n","When is the F-distribution used in hypothesis testing?\n","\n","The F-distribution is primarily used in:\n","\n","Analysis of Variance (ANOVA): To test whether the means of multiple groups are significantly different.\n","Regression Analysis: To assess the overall significance of a regression model.\n","Equality of Variances (F-test): To compare the variances of two populations.\n","\n","In hypothesis testing, the F-statistic is calculated as the ratio of two sample variances: $$ F = \\frac{\\text{Variance of group 1}}{\\text{Variance of group 2}} $$ The null hypothesis typically assumes that the variances (or other quantities being compared) are equal.\n","\n","Using the F-distribution in Python\n","\n","Python provides tools for working with the F-distribution through libraries like scipy and statsmodels. Here's how you can use it:\n","\n","1. Performing an F-test for Equality of Variances\n","Copy the code\n","from scipy.stats import f\n","\n","# Example: Variances of two groups\n","var1 = 4.5  # Variance of group 1\n","var2 = 2.3  # Variance of group 2\n","df1 = 10    # Degrees of freedom for group 1\n","df2 = 12    # Degrees of freedom for group 2\n","\n","# Calculate the F-statistic\n","f_statistic = var1 / var2\n","\n","# Calculate the p-value\n","p_value = 1 - f.cdf(f_statistic, df1, df2)\n","\n","print(f\"F-statistic: {f_statistic}\")\n","print(f\"P-value: {p_value}\")\n","\n","2. One-Way ANOVA\n","Copy the code\n","from scipy.stats import f_oneway\n","\n","# Example: Data from three groups\n","group1 = [12, 14, 15, 16, 19]\n","group2 = [22, 24, 25, 27, 30]\n","group3 = [32, 34, 35, 37, 40]\n","\n","# Perform one-way ANOVA\n","f_statistic, p_value = f_oneway(group1, group2, group3)\n","\n","print(f\"F-statistic: {f_statistic}\")\n","print(f\"P-value: {p_value}\")\n","\n","3. F-distribution Visualization\n","Copy the code\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import f\n","\n","# Parameters for F-distribution\n","df1, df2 = 5, 10\n","x = np.linspace(0, 5, 500)\n","y = f.pdf(x, df1, df2)\n","\n","# Plot the F-distribution\n","plt.plot(x, y, label=f\"F-distribution (df1={df1}, df2={df2})\")\n","plt.title(\"F-distribution\")\n","plt.xlabel(\"F-value\")\n","plt.ylabel(\"Probability Density\")\n","plt.legend()\n","plt.show()\n","\n","Summary\n","\n","The F-distribution is a cornerstone of statistical hypothesis testing when comparing variances or testing the significance of models. Python's scipy library makes it straightforward to calculate F-statistics, p-values, and visualize the distribution."],"metadata":{"id":"RDSVgtCPkMya"}},{"cell_type":"markdown","source":["20. What is an ANOVA test, and what are its assumptions?\n","-ANOVA‚Äîshort for Analysis of Variance‚Äîis a statistical test used when you want to compare the means of three or more groups to see if at least one of them is significantly different. Instead of running multiple t-tests (which increase the risk of Type I error), ANOVA does it all in one go.\n","\n","üéØ What Does ANOVA Do?\n","It analyzes how much of the total variance in your data can be attributed to between-group differences versus within-group variation. The output is an F-statistic and a p-value:\n","\n","A large F and small p-value (typically < 0.05) suggest at least one group mean is significantly different.\n","üß™ Key Assumptions of ANOVA\n","Independence: The observations in each group are independent.\n","\n","Normality: Each group‚Äôs data is approximately normally distributed.\n","\n","Homogeneity of variance (homoscedasticity): The variances among"],"metadata":{"id":"o-Ycww4fkclL"}},{"cell_type":"markdown","source":["21. What are the different types of ANOVA tests?\n","-ANOVA, short for Analysis of Variance, is a powerful statistical method used to compare the means of three or more groups. Depending on your experimental design and the number of variables you're comparing, there are several types of ANOVA tests‚Äîeach with its own flavor and purpose.\n","üß™ Common Types of ANOVA in Python\n","1. One-Way ANOVA\n","Purpose: Tests whether the means of three or more independent groups differ.\n","\n","Assumption: One categorical independent variable, one continuous dependent variable.\n","\n","Python Example (using scipy.stats.f_oneway):\n","from scipy.stats import f_oneway\n","\n","group1 = [21, 22, 19, 24]\n","group2 = [30, 29, 33, 28]\n","group3 = [25, 27, 26, 30]\n","\n","f_stat, p_val = f_oneway(group1, group2, group3)\n","print(\"F-statistic:\", f_stat)\n","print(\"P-value:\", p_val)\n","2. Two-Way ANOVA\n","Purpose: Tests the effect of two independent variables (factors) on a dependent variable, and checks for interaction effects.\n","\n","Requires: More structured data (often in a DataFrame).\n","\n","Python Tool: statsmodels\n","import pandas as pd\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","# Example DataFrame\n","df = pd.DataFrame({\n","    'score': [91, 87, 89, 95, 85, 88],\n","    'treatment': ['A', 'A', 'B', 'B', 'C', 'C'],\n","    'location': ['X', 'Y', 'X', 'Y', 'X', 'Y']\n","})\n","\n","model = ols('score ~ C(treatment) + C(location) + C(treatment):C(location)', data=df).fit()\n","anova_table = sm.stats.anova_lm(model, typ=2)\n","print(anova_table)\n","3. Repeated Measures ANOVA\n","Purpose: Tests differences when the same subjects are measured multiple times (e.g., before, during, and after treatment).\n","\n","Python Tool: statsmodels or pingouin\n","import pingouin as pg\n","df = pg.read_dataset(\"rm_anova\")  # Built-in example dataset\n","aov = pg.rm_anova(dv='Scores', within='Time', subject='Subject', data=df)\n","print(aov)\n","4. MANOVA (Multivariate ANOVA)\n","Purpose: Compares groups across multiple dependent variables simultaneously.\n","\n","Python Tool: statsmodels\n","from statsmodels.multivariate.manova import MANOVA\n","\n","df = pd.DataFrame({\n","    'group': ['A', 'A', 'B', 'B'],\n","    'math': [90, 85, 88, 92],\n","    'science': [88, 84, 89, 91]\n","})\n","\n","maov = MANOVA.from_formula('math + science ~ group', data=df)\n","print(maov.mv_test())\n","Each type of ANOVA peels back a different layer of data structure and relationships."],"metadata":{"id":"idhDdx5DkvRB"}},{"cell_type":"markdown","source":["22. What is the F-test, and how does it relate to hypothesis testing?\n","-The F-test is a statistical test used to compare two variances or to assess whether a group of variables significantly explains the variability of a response. It‚Äôs an integral part of ANOVA (Analysis of Variance) and regression analysis in hypothesis testing.\n","\n","üéØ What Does the F-Test Do?\n","At its core, the F-test evaluates:\n","\n","Are group means different? (in ANOVA)\n","\n","Is the variance explained by the model significant? (in linear regression)\n","\n","Do two populations have equal variances? (in a comparison of variances)\n","\n","The test statistic follows an F-distribution, which is right-skewed and depends on two different degrees of freedom: one for the numerator and one for the denominator.\n","üß™ Python Example: F-Test for Equal Variances\n","Using scipy.stats‚Äôs f_oneway for ANOVA:\n","from scipy.stats import f_oneway\n","\n","# Three groups\n","group1 = [20, 22, 19, 24, 20]\n","group2 = [30, 29, 33, 31, 32]\n","group3 = [25, 28, 27, 26, 29]\n","\n","f_stat, p_val = f_oneway(group1, group2, group3)\n","print(\"F-statistic:\", f_stat)\n","print(\"P-value:\", p_val)\n","If the p-value is less than your chosen Œ± (e.g., 0.05), you reject the null hypothesis that all group means are equal.\n","\n","The F-statistic quantifies the ratio of variation between groups to variation within groups.\n"],"metadata":{"id":"u_8hMRzKlY-c"}},{"cell_type":"markdown","source":["PRACTICAL----------"],"metadata":{"id":"oBN23yLLl0ft"}},{"cell_type":"code","source":["1.Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and\n","interpret the results?\n","-Here‚Äôs a Python program to perform a z-test for comparing a sample mean to a known population mean. The program calculates the z-score and p-value, and interprets the results based on a significance level (commonly 0.05).\n","\n","Copy the code\n","import scipy.stats as stats\n","import math\n","\n","# Function to perform a z-test\n","def z_test(sample_mean, population_mean, population_std, sample_size, alpha=0.05):\n","    # Calculate the standard error\n","    standard_error = population_std / math.sqrt(sample_size)\n","\n","    # Calculate the z-score\n","    z_score = (sample_mean - population_mean) / standard_error\n","\n","    # Calculate the p-value (two-tailed test)\n","    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n","\n","    # Interpret the results\n","    print(f\"Z-Score: {z_score:.4f}\")\n","    print(f\"P-Value: {p_value:.4f}\")\n","\n","    if p_value < alpha:\n","        print(\"Result: Reject the null hypothesis. The sample mean is significantly different from the population mean.\")\n","    else:\n","        print(\"Result: Fail to reject the null hypothesis. The sample mean is not significantly different from the population mean.\")\n","\n","# Example usage\n","sample_mean = 105  # Example sample mean\n","population_mean = 100  # Known population mean\n","population_std = 15  # Known population standard deviation\n","sample_size = 30  # Size of the sample\n","alpha = 0.05  # Significance level\n","\n","z_test(sample_mean, population_mean, population_std, sample_size, alpha)\n","\n","Explanation:\n","\n","Inputs:\n","\n","sample_mean: The mean of the sample.\n","population_mean: The known mean of the population.\n","population_std: The known standard deviation of the population.\n","sample_size: The size of the sample.\n","alpha: The significance level (default is 0.05).\n","\n","Steps:\n","\n","Calculate the standard error: $$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n","Compute the z-score: $$Z = \\frac{\\bar{x} - \\mu}{SE}$$\n","Determine the p-value for a two-tailed test.\n","Compare the p-value to the significance level to decide whether to reject the null hypothesis.\n","\n","Output:\n","\n","The z-score and p-value are displayed.\n","The program interprets the results, indicating whether the sample mean is significantly different from the population mean.\n","\n","This program is adaptable for various datasets‚Äîjust update the input values accordingly!"],"metadata":{"id":"KKHBp5Djl3LM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python?\n","-Here‚Äôs a concise Python example to simulate random data, perform hypothesis testing, and calculate the corresponding p-value using the scipy.stats module:\n","\n","Copy the code\n","import numpy as np\n","from scipy.stats import ttest_ind\n","\n","# Simulate random data for two groups\n","np.random.seed(42)  # For reproducibility\n","group1 = np.random.normal(loc=50, scale=10, size=100)  # Mean=50, Std=10, Size=100\n","group2 = np.random.normal(loc=55, scale=10, size=100)  # Mean=55, Std=10, Size=100\n","\n","# Perform two-sample t-test\n","t_stat, p_value = ttest_ind(group1, group2)\n","\n","# Display results\n","print(f\"T-statistic: {t_stat:.4f}\")\n","print(f\"P-value: {p_value:.4f}\")\n","\n","Explanation:\n","Data Simulation: Two random datasets (group1 and group2) are generated using a normal distribution with specified means, standard deviations, and sizes.\n","Hypothesis Testing: A two-sample t-test is performed to compare the means of the two groups.\n","Output: The t-statistic and p-value are printed, which help determine whether to reject the null hypothesis.\n","\n","This example is adaptable for other hypothesis tests (e.g., ANOVA, chi-square) by modifying the statistical test and data generation."],"metadata":{"id":"Jib-AGikmFZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean?\n","-Here is an implementation of a one-sample Z-test in Python to compare a sample mean with a population mean:\n","\n","Copy the code\n","import numpy as np\n","from scipy.stats import norm\n","\n","def one_sample_z_test(sample, population_mean, population_std):\n","    # Calculate sample mean\n","    sample_mean = np.mean(sample)\n","\n","    # Calculate sample size\n","    n = len(sample)\n","\n","    # Calculate the Z-score\n","    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(n))\n","\n","    # Calculate the p-value (two-tailed test)\n","    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n","\n","    return z_score, p_value\n","\n","# Example usage\n","sample = [50, 52, 48, 49, 51, 50, 53]  # Sample data\n","population_mean = 50  # Population mean\n","population_std = 2  # Population standard deviation (known)\n","\n","z_score, p_value = one_sample_z_test(sample, population_mean, population_std)\n","\n","print(f\"Z-Score: {z_score}\")\n","print(f\"P-Value: {p_value}\")\n","\n","# Interpretation\n","if p_value < 0.05:  # Assuming a significance level of 0.05\n","    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n","else:\n","    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")\n","\n","Explanation:\n","\n","Inputs:\n","\n","sample: The sample data.\n","population_mean: The known population mean.\n","population_std: The known population standard deviation.\n","\n","Steps:\n","\n","Compute the sample mean.\n","Calculate the Z-score using the formula: $$ Z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} $$ where:\n","$$\\bar{x}$$ is the sample mean,\n","$$\\mu$$ is the population mean,\n","$$\\sigma$$ is the population standard deviation,\n","$$n$$ is the sample size.\n","Compute the p-value for a two-tailed test using the cumulative distribution function (CDF) of the standard normal distribution.\n","\n","Output:\n","\n","z_score: The Z-statistic.\n","p_value: The probability of observing the sample mean under the null hypothesis.\n","\n","This code is adaptable for different datasets and hypotheses."],"metadata":{"id":"ofbBET8PmQ_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot?\n","-Here‚Äôs how you can perform a two-tailed Z-test in Python and visualize the decision region on a plot. This example assumes you have a sample mean, population mean, population standard deviation, and sample size.\n","\n","Code Example\n","Copy the code\n","import numpy as np\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","\n","# Parameters\n","population_mean = 50  # Hypothesized population mean (H0)\n","population_std = 10   # Population standard deviation\n","sample_mean = 53      # Sample mean\n","sample_size = 30      # Sample size\n","alpha = 0.05          # Significance level\n","\n","# Z-test calculation\n","z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n","critical_value = stats.norm.ppf(1 - alpha / 2)  # Two-tailed critical value\n","\n","# Decision\n","if abs(z_score) > critical_value:\n","    decision = \"Reject the null hypothesis (H0)\"\n","else:\n","    decision = \"Fail to reject the null hypothesis (H0)\"\n","\n","print(f\"Z-Score: {z_score:.2f}\")\n","print(f\"Critical Value: ¬±{critical_value:.2f}\")\n","print(f\"Decision: {decision}\")\n","\n","# Visualization of decision region\n","x = np.linspace(-4, 4, 1000)\n","y = stats.norm.pdf(x, 0, 1)\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(x, y, label=\"Standard Normal Distribution\", color=\"blue\")\n","\n","# Shade rejection regions\n","plt.fill_between(x, y, where=(x < -critical_value), color=\"red\", alpha=0.5, label=\"Rejection Region\")\n","plt.fill_between(x, y, where=(x > critical_value), color=\"red\", alpha=0.5)\n","\n","# Mark z-score\n","plt.axvline(z_score, color=\"green\", linestyle=\"--\", label=f\"Z-Score = {z_score:.2f}\")\n","\n","# Add labels and legend\n","plt.title(\"Two-Tailed Z-Test Decision Region\")\n","plt.xlabel(\"Z-Score\")\n","plt.ylabel(\"Probability Density\")\n","plt.axvline(-critical_value, color=\"black\", linestyle=\"--\", label=f\"Critical Value = ¬±{critical_value:.2f}\")\n","plt.axvline(critical_value, color=\"black\", linestyle=\"--\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","Explanation:\n","\n","Z-Score Calculation:\n","\n","$$ Z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} $$\n","Where:\n","$$ \\bar{x} $$ = Sample mean\n","$$ \\mu $$ = Population mean\n","$$ \\sigma $$ = Population standard deviation\n","$$ n $$ = Sample size\n","\n","Critical Value:\n","\n","For a two-tailed test, the critical value is determined using the standard normal distribution and the significance level $$ \\alpha $$.\n","\n","Decision:\n","\n","If the absolute value of the Z-score exceeds the critical value, reject the null hypothesis.\n","\n","Visualization:\n","\n","The plot shows the standard normal distribution with shaded rejection regions and the calculated Z-score.\n","\n","This code is adaptable for different parameters and provides a clear visual representation of the decision-making process."],"metadata":{"id":"9itqtkwQmev2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing?\n","-Here's a Python function that calculates and visualizes Type I and Type II errors during hypothesis testing. The function uses a normal distribution to simulate the null and alternative hypotheses and plots the results for better understanding.\n","\n","Copy the code\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","\n","def visualize_hypothesis_testing(mu_null=0, mu_alt=1, sigma=1, alpha=0.05, sample_size=30):\n","    \"\"\"\n","    Visualizes Type I and Type II errors in hypothesis testing.\n","\n","    Parameters:\n","    - mu_null: Mean of the null hypothesis (H0).\n","    - mu_alt: Mean of the alternative hypothesis (H1).\n","    - sigma: Standard deviation of the population.\n","    - alpha: Significance level (Type I error rate).\n","    - sample_size: Number of samples used in the test.\n","    \"\"\"\n","    # Calculate the critical value (z-score) for the significance level\n","    z_critical = norm.ppf(1 - alpha)\n","    critical_value = mu_null + z_critical * (sigma / np.sqrt(sample_size))\n","\n","    # Generate x values for the distributions\n","    x = np.linspace(mu_null - 4 * sigma, mu_alt + 4 * sigma, 1000)\n","\n","    # Null hypothesis distribution (H0)\n","    null_dist = norm.pdf(x, mu_null, sigma / np.sqrt(sample_size))\n","\n","    # Alternative hypothesis distribution (H1)\n","    alt_dist = norm.pdf(x, mu_alt, sigma / np.sqrt(sample_size))\n","\n","    # Plot the distributions\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(x, null_dist, label=\"Null Hypothesis (H0)\", color=\"blue\")\n","    plt.plot(x, alt_dist, label=\"Alternative Hypothesis (H1)\", color=\"red\")\n","\n","    # Shade the Type I error region\n","    plt.fill_between(x, 0, null_dist, where=(x > critical_value), color=\"blue\", alpha=0.3, label=\"Type I Error (Œ±)\")\n","\n","    # Shade the Type II error region\n","    plt.fill_between(x, 0, alt_dist, where=(x <= critical_value), color=\"red\", alpha=0.3, label=\"Type II Error (Œ≤)\")\n","\n","    # Add labels and legend\n","    plt.axvline(critical_value, color=\"black\", linestyle=\"--\", label=f\"Critical Value = {critical_value:.2f}\")\n","    plt.title(\"Type I and Type II Errors in Hypothesis Testing\")\n","    plt.xlabel(\"Test Statistic\")\n","    plt.ylabel(\"Probability Density\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","# Example usage\n","visualize_hypothesis_testing(mu_null=0, mu_alt=1, sigma=1, alpha=0.05, sample_size=30)\n","\n","Explanation:\n","\n","Inputs:\n","\n","mu_null: Mean of the null hypothesis distribution.\n","mu_alt: Mean of the alternative hypothesis distribution.\n","sigma: Standard deviation of the population.\n","alpha: Significance level (probability of Type I error).\n","sample_size: Number of samples used in the test.\n","\n","Critical Value:\n","\n","The critical value is calculated based on the significance level and the null hypothesis distribution.\n","\n","Visualization:\n","\n","The null hypothesis (H0) and alternative hypothesis (H1) distributions are plotted.\n","The Type I error region (false rejection of H0) is shaded in blue.\n","The Type II error region (false acceptance of H0) is shaded in red.\n","\n","This function provides an intuitive way to understand the trade-offs between Type I and Type II errors in hypothesis testing."],"metadata":{"id":"uC9_rSximu0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["6. Write a Python program to perform an independent T-test and interpret the results?\n","-Absolutely! An independent t-test compares the means of two independent groups to determine if there's a statistically significant difference between them. Here's a Python example with a walkthrough:\n","from scipy import stats\n","\n","# Sample data: scores from two different groups\n","group_a = [85, 88, 90, 94, 87]\n","group_b = [78, 80, 82, 76, 79]\n","\n","# Perform the independent t-test\n","t_stat, p_val = stats.ttest_ind(group_a, group_b)\n","\n","# Display the results\n","print(f\"T-statistic: {t_stat:.3f}\")\n","print(f\"P-value: {p_val:.3f}\")\n","\n","# Interpretation\n","alpha = 0.05\n","if p_val < alpha:\n","    print(\"Result: Statistically significant difference between groups (Reject H‚ÇÄ)\")\n","else:\n","    print(\"Result: No statistically significant difference (Fail to reject H‚ÇÄ)\")\n","üß† How to Interpret the Output\n","Let‚Äôs say you get this output:\n","T-statistic: 5.247\n","P-value: 0.001\n","Result: Statistically significant difference between groups (Reject H‚ÇÄ)\n","This means:\n","\n","The means are likely different.\n","\n","With p-value = 0.001 < Œ± = 0.05, you reject the null hypothesis that the group means are equal.\n","\n","Group A likely performed better than Group B (you can verify this by comparing their means)."],"metadata":{"id":"cTUeijPgm-ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["7. Perform a paired sample T-test using Python and visualize the comparison results?\n","-Here‚Äôs how you can perform a paired sample t-test in Python and visualize the comparison results. I'll provide a concise example:\n","\n","Code Example\n","Copy the code\n","import numpy as np\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","\n","# Example data: Before and After measurements\n","before = np.array([85, 90, 88, 75, 95, 80, 78, 92, 88, 84])\n","after = np.array([88, 92, 85, 78, 97, 82, 80, 94, 90, 86])\n","\n","# Perform paired sample t-test\n","t_stat, p_value = stats.ttest_rel(before, after)\n","\n","# Print results\n","print(f\"T-statistic: {t_stat:.3f}\")\n","print(f\"P-value: {p_value:.3f}\")\n","\n","# Visualization\n","plt.figure(figsize=(8, 5))\n","x = np.arange(len(before)) + 1  # Index for each pair\n","\n","plt.plot(x, before, 'o-', label='Before', color='blue')\n","plt.plot(x, after, 'o-', label='After', color='green')\n","plt.fill_between(x, before, after, color='gray', alpha=0.2, label='Difference')\n","plt.axhline(y=np.mean(before), color='blue', linestyle='--', label='Mean Before')\n","plt.axhline(y=np.mean(after), color='green', linestyle='--', label='Mean After')\n","\n","plt.title('Paired Sample Comparison')\n","plt.xlabel('Sample Index')\n","plt.ylabel('Values')\n","plt.legend()\n","plt.grid(alpha=0.3)\n","plt.show()\n","\n","Explanation\n","Data: before and after arrays represent paired measurements.\n","Paired t-test: stats.ttest_rel() is used for paired sample t-tests.\n","Visualization:\n","Line plots show the \"before\" and \"after\" values for each pair.\n","Shaded areas highlight the differences.\n","Dashed lines represent the mean of each group.\n","\n","This approach provides both statistical results and a clear visual comparison of the paired data."],"metadata":{"id":"lPAe1fgsnjET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["8. Simulate data and perform both Z-test and T-test, then compare the results using Python.\n","-Absolutely! Let‚Äôs walk through how to simulate data, perform both a Z-test and a T-test, and compare the results using Python. This is a great way to see how these tests behave under different assumptions.\n","\n","üß™ Step 1: Simulate Data\n","import numpy as np\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","# Simulate two independent samples\n","group1 = np.random.normal(loc=100, scale=10, size=50)  # mean=100, std=10\n","group2 = np.random.normal(loc=105, scale=10, size=50)  # mean=105, std=10\n","üìä Step 2: Perform a T-Test (population std unknown)\n","from scipy import stats\n","\n","t_stat, t_pval = stats.ttest_ind(group1, group2)\n","print(f\"T-test ‚Üí t-statistic: {t_stat:.3f}, p-value: {t_pval:.3f}\")\n","üìà Step 3: Perform a Z-Test (assuming known population std)\n","import statsmodels.stats.weightstats as sm\n","\n","# Assume known population std = 10\n","z_stat, z_pval = sm.ztest(group1, group2, value=0, alternative='two-sided')\n","print(f\"Z-test ‚Üí z-statistic: {z_stat:.3f}, p-value: {z_pval:.3f}\")\n","üîç Step 4: Compare the Results\n","T-test is more conservative when sample size is small or population std is unknown.\n","\n","Z-test assumes known population std and is more sensitive with large samples.\n","\n","If both tests give similar p-values, it suggests your sample size is large enough for the t-distribution to approximate the normal distribution.\n"],"metadata":{"id":"e1GD6jGNnyC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.?\n","-üßÆ Python Function: Confidence Interval for Sample Mean\n","import numpy as np\n","from scipy import stats\n","\n","def confidence_interval(data, confidence=0.95):\n","    \"\"\"\n","    Calculate the confidence interval for a sample mean.\n","\n","    Parameters:\n","        data (list or array-like): Sample data\n","        confidence (float): Confidence level (default is 0.95)\n","\n","    Returns:\n","        tuple: (mean, lower bound, upper bound)\n","    \"\"\"\n","    data = np.array(data)\n","    n = len(data)\n","    mean = np.mean(data)\n","    std_err = stats.sem(data)  # Standard error of the mean\n","    margin = std_err * stats.t.ppf((1 + confidence) / 2, df=n-1)\n","    return mean, mean - margin, mean + margin\n","‚úÖ Example Usage-\n","sample = [72, 75, 78, 71, 76, 77, 74]\n","mean, lower, upper = confidence_interval(sample)\n","\n","print(f\"Sample Mean: {mean:.2f}\")\n","print(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f})\")\n","üìå What It Means\n","A 95% confidence interval means: > If you repeated your sampling many times, about 95% of those intervals would contain the true population mean.\n","\n","It gives a range of plausible values for the true average, accounting for sample size and variability. Wider intervals imply more uncertainty, while narrower ones suggest more precision."],"metadata":{"id":"GMBgrqjmoa22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["10.Write a Python program to calculate the margin of error for a given confidence level using sample data?\n","-Here‚Äôs a Python program to calculate the margin of error for a given confidence level using sample data. The program uses the z-score for the confidence level and assumes a normal distribution.\n","\n","Copy the code\n","import math\n","from scipy.stats import norm\n","\n","def calculate_margin_of_error(sample_mean, sample_std_dev, sample_size, confidence_level):\n","    # Calculate the z-score for the given confidence level\n","    z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n","\n","    # Calculate the standard error\n","    standard_error = sample_std_dev / math.sqrt(sample_size)\n","\n","    # Calculate the margin of error\n","    margin_of_error = z_score * standard_error\n","\n","    return margin_of_error\n","\n","# Example usage\n","sample_mean = 50  # Example sample mean\n","sample_std_dev = 10  # Example sample standard deviation\n","sample_size = 100  # Example sample size\n","confidence_level = 0.95  # 95% confidence level\n","\n","margin_of_error = calculate_margin_of_error(sample_mean, sample_std_dev, sample_size, confidence_level)\n","print(f\"Margin of Error: {margin_of_error:.2f}\")\n","\n","Explanation:\n","\n","Inputs:\n","\n","sample_mean: The mean of the sample data (not directly used in margin of error calculation but often part of the context).\n","sample_std_dev: The standard deviation of the sample data.\n","sample_size: The size of the sample.\n","confidence_level: The desired confidence level (e.g., 0.95 for 95%).\n","\n","Steps:\n","\n","The z-score is calculated using the scipy.stats.norm.ppf function.\n","The standard error is computed as $$\\text{standard error} = \\frac{\\text{sample standard deviation}}{\\sqrt{\\text{sample size}}}$$.\n","The margin of error is then $$\\text{margin of error} = \\text{z-score} \\times \\text{standard error}$$.\n","\n","Output:\n","\n","The program prints the margin of error rounded to two decimal places.\n","\n","You can modify the inputs (sample_mean, sample_std_dev, sample_size, and confidence_level) to suit your specific data."],"metadata":{"id":"eRZPYmmHo2Z7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n","-Here‚Äôs an implementation of Bayesian inference using Bayes' Theorem in Python, along with an explanation of the process:\n","\n","Bayes' Theorem\n","\n","Bayes' Theorem is expressed as:\n","\n","$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n","\n","Where:\n","\n","P(A|B): Posterior probability (probability of A given B).\n","P(B|A): Likelihood (probability of B given A).\n","P(A): Prior probability (initial belief about A).\n","P(B): Evidence (normalizing constant).\n","Python Implementation\n","Copy the code\n","# Define the probabilities\n","def bayesian_inference(prior_A, likelihood_B_given_A, likelihood_B_given_not_A):\n","    \"\"\"\n","    Perform Bayesian inference using Bayes' Theorem.\n","\n","    Parameters:\n","    - prior_A: P(A) - Prior probability of A\n","    - likelihood_B_given_A: P(B|A) - Likelihood of B given A\n","    - likelihood_B_given_not_A: P(B|¬¨A) - Likelihood of B given not A\n","\n","    Returns:\n","    - posterior_A_given_B: P(A|B) - Posterior probability of A given B\n","    \"\"\"\n","    # Calculate P(¬¨A) (complement of A)\n","    prior_not_A = 1 - prior_A\n","\n","    # Calculate P(B) (evidence)\n","    evidence_B = (likelihood_B_given_A * prior_A) + (likelihood_B_given_not_A * prior_not_A)\n","\n","    # Calculate P(A|B) (posterior probability)\n","    posterior_A_given_B = (likelihood_B_given_A * prior_A) / evidence_B\n","\n","    return posterior_A_given_B\n","\n","\n","# Example usage\n","prior_A = 0.01  # Prior probability of having a disease\n","likelihood_B_given_A = 0.9  # Probability of testing positive if diseased\n","likelihood_B_given_not_A = 0.05  # Probability of testing positive if not diseased\n","\n","posterior = bayesian_inference(prior_A, likelihood_B_given_A, likelihood_B_given_not_A)\n","print(f\"Posterior Probability (P(A|B)): {posterior:.4f}\")\n","\n","Explanation of the Process\n","\n","Define Inputs:\n","\n","prior_A: The initial belief about the probability of event A (e.g., having a disease).\n","likelihood_B_given_A: The probability of observing evidence B (e.g., a positive test result) if A is true.\n","likelihood_B_given_not_A: The probability of observing evidence B if A is false.\n","\n","Calculate Complement:\n","\n","Compute the probability of the complement of A, i.e., $$P(\\neg A) = 1 - P(A)$$.\n","\n","Compute Evidence:\n","\n","The evidence $$P(B)$$ is the total probability of observing B, considering both scenarios (A and ¬¨A): $$ P(B) = P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A) $$\n","\n","Apply Bayes' Theorem:\n","\n","Use the formula to compute the posterior probability: $$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n","\n","Output the Result:\n","\n","The result is the updated belief (posterior probability) about A after observing B.\n","Example Scenario\n","Prior: 1% of the population has a disease.\n","Likelihood: A test detects the disease 90% of the time if present, but has a 5% false positive rate.\n","Posterior: The probability of having the disease given a positive test result is calculated.\n","\n","This approach can be adapted to various Bayesian inference problems by modifying the inputs."],"metadata":{"id":"pE3HYig2p_bC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["12. Perform a Chi-square test for independence between two categorical variables in Python?\n","-To perform a Chi-Square test for independence between two categorical variables in Python, you can use the chi2_contingency function from the scipy.stats module. Here's an example:\n","\n","Example Code:\n","Copy the code\n","import numpy as np\n","from scipy.stats import chi2_contingency\n","\n","# Create a contingency table (example data)\n","# Rows and columns represent the categories of the two variables\n","data = np.array([[50, 30, 20],  # Category 1 of Variable A\n","                 [30, 50, 20],  # Category 2 of Variable A\n","                 [20, 20, 60]]) # Category 3 of Variable A\n","\n","# Perform the Chi-Square test\n","chi2, p, dof, expected = chi2_contingency(data)\n","\n","# Print the results\n","print(\"Chi-Square Statistic:\", chi2)\n","print(\"P-value:\", p)\n","print(\"Degrees of Freedom:\", dof)\n","print(\"Expected Frequencies:\\n\", expected)\n","\n","# Interpretation\n","if p < 0.05:\n","    print(\"The variables are likely dependent (reject the null hypothesis).\")\n","else:\n","    print(\"The variables are likely independent (fail to reject the null hypothesis).\")\n","\n","Explanation:\n","Input Data: The data array is a contingency table where rows and columns represent the categories of the two variables.\n","Chi-Square Test: The chi2_contingency function computes:\n","chi2: The test statistic.\n","p: The p-value.\n","dof: Degrees of freedom.\n","expected: The expected frequencies under the null hypothesis.\n","Interpretation: Compare the p-value to your significance level (commonly 0.05) to determine if the variables are independent.\n","\n","This method is flexible and works for any contingency table. Just replace the data array with your own dataset!"],"metadata":{"id":"VSstHIeZqTZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["13.Write a Python program to calculate the expected frequencies for a Chi-square test based on observed\n","data?\n","-Here is a Python program to calculate the expected frequencies for a Chi-Square test based on observed data. The program assumes you have a contingency table (observed data) and calculates the expected frequencies using the formula:\n","\n","$$E_{ij} = \\frac{(R_i \\cdot C_j)}{N}$$\n","\n","Where:\n","\n","$$E_{ij}$$ is the expected frequency for cell (i, j),\n","$$R_i$$ is the sum of the i-th row,\n","$$C_j$$ is the sum of the j-th column,\n","$$N$$ is the total sum of all observations.\n","Copy the code\n","import numpy as np\n","\n","def calculate_expected_frequencies(observed):\n","    # Convert observed data to a NumPy array for easier calculations\n","    observed = np.array(observed)\n","\n","    # Calculate row sums, column sums, and total sum\n","    row_sums = observed.sum(axis=1)\n","    col_sums = observed.sum(axis=0)\n","    total = observed.sum()\n","\n","    # Calculate expected frequencies\n","    expected = np.outer(row_sums, col_sums) / total\n","\n","    return expected\n","\n","# Example observed data (contingency table)\n","observed_data = [\n","    [50, 30, 20],\n","    [30, 40, 30],\n","    [20, 30, 50]\n","]\n","\n","# Calculate expected frequencies\n","expected_frequencies = calculate_expected_frequencies(observed_data)\n","\n","# Print the result\n","print(\"Observed Data:\")\n","print(np.array(observed_data))\n","print(\"\\nExpected Frequencies:\")\n","print(expected_frequencies)\n","\n","Explanation:\n","Input: The observed_data is a 2D list representing the contingency table.\n","Row and Column Sums: The program calculates the sums of rows and columns using NumPy's sum function.\n","Expected Frequencies: The expected frequencies are calculated using the formula and stored in a 2D array.\n","Output: Both the observed data and expected frequencies are printed for comparison.\n","\n","You can replace the observed_data with your own contingency table to calculate expected frequencies for your specific dataset."],"metadata":{"id":"daGc2W2qqiGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution?\n","-To perform a goodness-of-fit test in Python, you can use the Chi-Square Goodness-of-Fit Test from the scipy.stats module. Here's an example:\n","\n","Example Code\n","Copy the code\n","import numpy as np\n","from scipy.stats import chisquare\n","\n","# Observed data\n","observed = np.array([50, 30, 20])\n","\n","# Expected data (must sum to the same total as observed)\n","expected = np.array([40, 40, 20])\n","\n","# Perform the Chi-Square Goodness-of-Fit Test\n","chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n","\n","# Output the results\n","print(f\"Chi-Square Statistic: {chi2_stat}\")\n","print(f\"P-Value: {p_value}\")\n","\n","# Interpretation\n","if p_value < 0.05:\n","    print(\"The observed data does not fit the expected distribution (reject null hypothesis).\")\n","else:\n","    print(\"The observed data fits the expected distribution (fail to reject null hypothesis).\")\n","\n","Key Notes:\n","\n","Inputs:\n","\n","f_obs: Observed frequencies (your data).\n","f_exp: Expected frequencies (theoretical distribution).\n","\n","Output:\n","\n","chi2_stat: The test statistic.\n","p_value: The probability of observing the data under the null hypothesis.\n","\n","Assumptions:\n","\n","Observed and expected frequencies should be non-negative.\n","Expected frequencies should not be too small (preferably > 5 for each category).\n","\n","This test helps determine whether the observed data significantly deviates from the expected distribution."],"metadata":{"id":"L3_6Emm7q4Ro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics?\n","-import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import chi2\n","\n","# Parameters\n","df_values = [1, 2, 5, 10]  # degrees of freedom\n","x = np.linspace(0, 30, 500)\n","\n","# Plot Chi-square PDFs for different degrees of freedom\n","plt.figure(figsize=(10, 6))\n","for df in df_values:\n","    plt.plot(x, chi2.pdf(x, df), label=f'df={df}')\n","\n","plt.title('Chi-square Distribution for Various Degrees of Freedom')\n","plt.xlabel('x')\n","plt.ylabel('Probability Density')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"WtIBLyeQrRUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["16. Implement an F-test using Python to compare the variances of two random samples?\n","-import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import chi2\n","\n","# Parameters\n","df_values = [1, 2, 5, 10]  # degrees of freedom\n","x = np.linspace(0, 30, 500)\n","\n","# Plot Chi-square PDFs for different degrees of freedom\n","plt.figure(figsize=(10, 6))\n","for df in df_values:\n","    plt.plot(x, chi2.pdf(x, df), label=f'df={df}')\n","\n","plt.title('Chi-square Distribution for Various Degrees of Freedom')\n","plt.xlabel('x')\n","plt.ylabel('Probability Density')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"p5xuTvqTALd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["17.Write a Python program to perform an ANOVA test to compare means between multiple groups and\n","interpret the results?\n","-from scipy.stats import f_oneway\n","\n","# Sample data: test scores from three different teaching methods\n","method_A = [85, 88, 90, 87, 86]\n","method_B = [78, 80, 82, 76, 79]\n","method_C = [92, 95, 91, 94, 93]\n","\n","# Perform one-way ANOVA\n","f_stat, p_val = f_oneway(method_A, method_B, method_C)\n","\n","print(f\"F-statistic: {f_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","\n","# Interpretation\n","alpha = 0.05\n","if p_val < alpha:\n","    print(\"Result: Statistically significant difference between group means (Reject H‚ÇÄ)\")\n","else:\n","    print(\"Result: No statistically significant difference (Fail to reject H‚ÇÄ)\")\n"],"metadata":{"id":"hieL4YZDAavP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["18.Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results?\n","-üß™ Step 1: Perform One-Way ANOVA\n","from scipy.stats import f_oneway\n","\n","# Sample data: test scores from three different teaching methods\n","group_A = [85, 88, 90, 87, 86]\n","group_B = [78, 80, 82, 76, 79]\n","group_C = [92, 95, 91, 94, 93]\n","\n","# Run ANOVA\n","f_stat, p_val = f_oneway(group_A, group_B, group_C)\n","\n","print(f\"F-statistic: {f_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","üìä Step 2: Visualize the Group Distributions\n","import matplotlib.pyplot as plt\n","\n","# Combine data for plotting\n","data = [group_A, group_B, group_C]\n","labels = ['Method A', 'Method B', 'Method C']\n","\n","plt.boxplot(data, labels=labels)\n","plt.title('Comparison of Teaching Methods')\n","plt.ylabel('Test Scores')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"atw7ZZCiAp1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA?\n","-üß™ Python Function to Check ANOVA Assumptions\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import shapiro, levene\n","from statsmodels.stats.diagnostic import acorr_ljungbox\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","def check_anova_assumptions(data, dv, iv):\n","    \"\"\"\n","    Check ANOVA assumptions: normality, independence, and equal variances.\n","\n","    Parameters:\n","        data (DataFrame): Your dataset\n","        dv (str): Name of the dependent variable column\n","        iv (str): Name of the independent variable column\n","    \"\"\"\n","    print(\"Fitting ANOVA model...\")\n","    model = ols(f'{dv} ~ C({iv})', data=data).fit()\n","    residuals = model.resid\n","\n","    # 1. Normality of residuals\n","    print(\"\\n1. Normality (Shapiro-Wilk Test):\")\n","    stat, p = shapiro(residuals)\n","    print(f\"Shapiro-Wilk p-value: {p:.4f}\")\n","    if p > 0.05:\n","        print(\"‚úÖ Residuals appear normally distributed.\")\n","    else:\n","        print(\"‚ö†Ô∏è Residuals may not be normally distributed.\")\n","\n","    # 2. Independence (Ljung-Box Test)\n","    print(\"\\n2. Independence (Ljung-Box Test):\")\n","    lb_stat, lb_p = acorr_ljungbox(residuals, lags=[1], return_df=False)\n","    print(f\"Ljung-Box p-value: {lb_p[0]:.4f}\")\n","    if lb_p[0] > 0.05:\n","        print(\"‚úÖ Residuals appear independent.\")\n","    else:\n","        print(\"‚ö†Ô∏è Residuals may not be independent.\")\n","\n","    # 3. Homogeneity of variances (Levene‚Äôs Test)\n","    print(\"\\n3. Equal Variance (Levene‚Äôs Test):\")\n","    groups = [group[dv].values for name, group in data.groupby(iv)]\n","    stat, p = levene(*groups)\n","    print(f\"Levene‚Äôs p-value: {p:.4f}\")\n","    if p > 0.05:\n","        print(\"‚úÖ Variances appear equal across groups.\")\n","    else:\n","        print(\"‚ö†Ô∏è Variances may not be equal.\")\n","\n","    # Optional: Residual plot\n","    sns.histplot(residuals, kde=True)\n","    plt.title(\"Histogram of Residuals\")\n","    plt.xlabel(\"Residuals\")\n","    plt.ylabel(\"Frequency\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"2D6_deYjBD1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["20.Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the\n","results?\n","-üß™ Step 1: Create or Load Your Data\n","import pandas as pd\n","\n","# Example dataset: Exam scores by teaching method and gender\n","data = pd.DataFrame({\n","    'score': [88, 92, 85, 90, 78, 82, 75, 80, 91, 89, 87, 93],\n","    'method': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],\n","    'gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F']\n","})\n","üìä Step 2: Perform Two-Way ANOVA\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","# Fit the model with interaction\n","model = ols('score ~ C(method) + C(gender) + C(method):C(gender)', data=data).fit()\n","anova_table = sm.stats.anova_lm(model, typ=2)\n","print(anova_table)\n","This will show you:\n","\n","The main effects of method and gender\n","\n","The interaction effect between them\n","üìà Step 3: Visualize the Interaction\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.pointplot(data=data, x='method', y='score', hue='gender', dodge=True, markers=['o', 's'], capsize=.1)\n","plt.title('Interaction Plot: Method vs Gender on Score')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"pziojEfqBZoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["21.Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing?\n","-üß™ Python Program: Visualize the F-Distribution\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import f\n","\n","# Define degrees of freedom\n","df1_values = [1, 2, 5, 10]\n","df2 = 20\n","x = np.linspace(0, 5, 500)\n","\n","# Plot F-distributions for different df1\n","plt.figure(figsize=(10, 6))\n","for df1 in df1_values:\n","    y = f.pdf(x, df1, df2)\n","    plt.plot(x, y, label=f'df1={df1}, df2={df2}')\n","\n","plt.title('F-Distribution for Various Degrees of Freedom')\n","plt.xlabel('F value')\n","plt.ylabel('Probability Density')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","üìå What the F-Distribution Tells Us\n","Right-skewed: Especially with small degrees of freedom.\n","\n","Used in ANOVA: To test if group means differ significantly.\n","\n","Used in regression: To test if the model explains a significant portion of variance.\n","\n","F-statistic: Ratio of variance between groups to variance within groups."],"metadata":{"id":"gG5JJve3B6Im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["22.Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means?\n","-üß™ Step 1: Perform One-Way ANOVA\n","from scipy.stats import f_oneway\n","\n","# Sample data: test scores from three different groups\n","group_A = [85, 88, 90, 87, 86]\n","group_B = [78, 80, 82, 76, 79]\n","group_C = [92, 95, 91, 94, 93]\n","\n","# Run ANOVA\n","f_stat, p_val = f_oneway(group_A, group_B, group_C)\n","\n","print(f\"F-statistic: {f_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","If the p-value < 0.05, you reject the null hypothesis and conclude that at least one group mean is significantly different.\n","üìä Step 2: Visualize with Boxplots\n","import matplotlib.pyplot as plt\n","\n","# Combine data for plotting\n","data = [group_A, group_B, group_C]\n","labels = ['Group A', 'Group B', 'Group C']\n","\n","plt.boxplot(data, labels=labels)\n","plt.title('Group Comparison via One-Way ANOVA')\n","plt.ylabel('Scores')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","Boxplots give a clear visual of the spread, median, and outliers for each group‚Äîperfect for spotting differences in central tendency and variability."],"metadata":{"id":"vR2-vrn1CK2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["23.Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means?\n","-üß™ Step-by-Step Python Code\n","import numpy as np\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","\n","# Step 1: Simulate random data from a normal distribution\n","np.random.seed(42)\n","sample = np.random.normal(loc=100, scale=10, size=50)  # mean=100, std=10\n","\n","# Step 2: Define population mean to test against\n","pop_mean = 105\n","\n","# Step 3: Perform one-sample t-test\n","t_stat, p_val = stats.ttest_1samp(sample, pop_mean)\n","\n","# Step 4: Print results\n","print(f\"Sample Mean: {np.mean(sample):.2f}\")\n","print(f\"T-statistic: {t_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","\n","# Step 5: Visualize the distribution\n","plt.hist(sample, bins=10, edgecolor='black', alpha=0.7)\n","plt.axvline(np.mean(sample), color='blue', linestyle='dashed', linewidth=2, label='Sample Mean')\n","plt.axvline(pop_mean, color='red', linestyle='dotted', linewidth=2, label='Population Mean')\n","plt.title('Simulated Normal Distribution')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","üß† Interpretation\n","If the p-value < 0.05, you reject the null hypothesis and conclude that the sample mean is significantly different from the population mean.\n","\n","The histogram helps visualize how the sample is distributed around the sample and population means."],"metadata":{"id":"fxR-h0lkCloo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results?\n","-üß™ Step-by-Step: Chi-Square Test for Population Variance\n","import numpy as np\n","from scipy.stats import chi2\n","\n","# Sample data\n","data = [12.5, 13.2, 11.8, 12.9, 13.5, 12.1, 13.0, 12.7]\n","n = len(data)\n","sample_var = np.var(data, ddof=1)  # Sample variance\n","hypothesized_var = 1.0             # Population variance under H‚ÇÄ\n","\n","# Chi-square test statistic\n","chi2_stat = (n - 1) * sample_var / hypothesized_var\n","\n","# Degrees of freedom\n","df = n - 1\n","\n","# Two-tailed p-value\n","p_val = 2 * min(chi2.cdf(chi2_stat, df), 1 - chi2.cdf(chi2_stat, df))\n","\n","print(f\"Sample Variance: {sample_var:.4f}\")\n","print(f\"Chi-square Statistic: {chi2_stat:.4f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","üß† Interpretation\n","Null Hypothesis (H‚ÇÄ): The population variance is equal to the hypothesized value.\n","\n","Alternative Hypothesis (H‚ÇÅ): The population variance is different.\n","\n","If the p-value < 0.05, reject H‚ÇÄ ‚Üí the sample provides evidence that the population variance is significantly different."],"metadata":{"id":"J73g0-yhC7bO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["25.Write a Python script to perform a Z-test for comparing proportions between two datasets or groups?\n","-üß™ Python Script: Two-Proportion Z-Test\n","import numpy as np\n","from statsmodels.stats.proportion import proportions_ztest\n","\n","# Example: Group A and Group B success counts and sample sizes\n","successes = np.array([45, 30])     # e.g., 45 successes in group A, 30 in group B\n","samples = np.array([100, 90])      # e.g., 100 trials in group A, 90 in group B\n","\n","# Perform two-proportion Z-test\n","z_stat, p_val = proportions_ztest(successes, samples)\n","\n","print(f\"Z-statistic: {z_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","\n","# Interpretation\n","alpha = 0.05\n","if p_val < alpha:\n","    print(\"Result: Statistically significant difference in proportions (Reject H‚ÇÄ)\")\n","else:\n","    print(\"Result: No significant difference in proportions (Fail to reject H‚ÇÄ)\")\n","üß† What This Means\n","Null Hypothesis (H‚ÇÄ): The proportions in both groups are equal.\n","\n","Alternative Hypothesis (H‚ÇÅ): The proportions are different.\n","\n","If the p-value < 0.05, you conclude there's a significant difference between the two proportions."],"metadata":{"id":"kV6JIUV0DNtv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["26.Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results?\n","-To compare the variances of two datasets using an F-test in Python, you can follow this step-by-step guide. We‚Äôll also visualize the distributions to better understand the variance differences.\n","üß™ Step 1: Simulate Two Datasets\n","import numpy as np\n","\n","np.random.seed(42)\n","group1 = np.random.normal(loc=50, scale=5, size=30)\n","group2 = np.random.normal(loc=50, scale=10, size=30)\n","üìä Step 2: Perform the F-Test\n","from scipy.stats import f\n","\n","# Calculate sample variances\n","var1 = np.var(group1, ddof=1)\n","var2 = np.var(group2, ddof=1)\n","\n","# F-statistic (larger variance / smaller variance)\n","f_stat = var1 / var2 if var1 > var2 else var2 / var1\n","\n","# Degrees of freedom\n","df1 = len(group1) - 1\n","df2 = len(group2) - 1\n","\n","# Two-tailed p-value\n","p_val = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n","\n","print(f\"F-statistic: {f_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","üìà Step 3: Visualize the Distributions\n","import matplotlib.pyplot as plt\n","\n","plt.hist(group1, bins=10, alpha=0.6, label='Group 1 (œÉ‚âà5)')\n","plt.hist(group2, bins=10, alpha=0.6, label='Group 2 (œÉ‚âà10)')\n","plt.axvline(np.mean(group1), color='blue', linestyle='dashed', label='Mean Group 1')\n","plt.axvline(np.mean(group2), color='orange', linestyle='dashed', label='Mean Group 2')\n","plt.title('Distribution Comparison')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","üß† Interpretation\n","Null Hypothesis (H‚ÇÄ): The two groups have equal variances.\n","\n","If p-value < 0.05, reject H‚ÇÄ ‚Üí the variances are significantly different.\n","\n","The histogram helps you visually confirm the spread difference between the two groups."],"metadata":{"id":"v_GrghzDDhSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n","-Absolutely! A Chi-square goodness-of-fit test checks whether observed categorical data matches an expected distribution. Let‚Äôs simulate some data, run the test, and interpret the results.\n","üß™ Step-by-Step Python Code\n","import numpy as np\n","from scipy.stats import chisquare\n","\n","# Step 1: Simulate observed frequencies (e.g., dice rolls)\n","observed = np.array([18, 22, 20, 16, 24, 20])  # 6-sided die rolled 120 times\n","\n","# Step 2: Define expected frequencies (uniform distribution)\n","expected = np.full_like(observed, fill_value=20)  # Expect 20 per face\n","\n","# Step 3: Perform Chi-square goodness-of-fit test\n","chi2_stat, p_val = chisquare(f_obs=observed, f_exp=expected)\n","\n","print(f\"Chi-square Statistic: {chi2_stat:.3f}\")\n","print(f\"P-value: {p_val:.4f}\")\n","üß† Interpretation\n","Null Hypothesis (H‚ÇÄ): The observed distribution matches the expected distribution.\n","\n","If p-value < 0.05, reject H‚ÇÄ ‚Üí the observed frequencies differ significantly from expected.\n","\n","If p-value ‚â• 0.05, fail to reject H‚ÇÄ ‚Üí no significant difference.\n","üìä Optional: Visualize Observed vs Expected\n","import matplotlib.pyplot as plt\n","\n","labels = ['1', '2', '3', '4', '5', '6']\n","x = np.arange(len(labels))\n","\n","plt.bar(x - 0.2, observed, width=0.4, label='Observed', color='skyblue')\n","plt.bar(x + 0.2, expected, width=0.4, label='Expected', color='orange')\n","plt.xticks(x, labels)\n","plt.xlabel('Die Face')\n","plt.ylabel('Frequency')\n","plt.title('Observed vs Expected Frequencies')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"v-IAvaXOEJlA"},"execution_count":null,"outputs":[]}]}